# ğŸ§  CLIP Embedding-Level Stroop Analysis

This repository evaluates how **CLIP** represents conflicting visual cues between **word meaning** and **ink color** in a Stroop-style task â€” both *numerically* (cosine similarity) and *geometrically* (UMAP visualization).

---

## ğŸ¯ Objective

When a word like **RED** is written in **blue ink**, does CLIPâ€™s embedding represent the **word** or the **color**?

We test this by comparing:
- The **image embedding** (from each Stroop image)  
- To the **text embeddings** of 10 color words  
  (`red, blue, green, yellow, orange, purple, brown, pink, gray, black`)

We then check which text embedding the image embedding is **closer** to â€”  
the *written word* or the *ink color*.

---

## ğŸ§© Method Overview

### Step 1 â€” Cosine Similarity Analysis
Each image embedding is compared to single-word text embeddings:

\[
\text{sim\_word} = \cos(f(I), g(\text{word}))
\quad\text{and}\quad
\text{sim\_ink} = \cos(f(I), g(\text{ink}))
\]

We record whether CLIPâ€™s image representation aligns more strongly with the **word** or **ink color**.

**Script:** `clip_stroop_embedding_analysis.py`  
**Output:** `results/clip_embedding_results.csv`

---

### Step 2 â€” UMAP Visualization
To visualize representation geometry, we project all **image** and **text** embeddings into 2D using [UMAP](https://umap-learn.readthedocs.io).

Each pair of embeddings is connected by a line:
- **â— Circles** = Image embeddings  
- **â–  Squares** = Text embeddings  
- **Colored lines** connect each image to its corresponding text prompt  
  (e.g., â€œThe word *RED* written in blue colorâ€)

**Script:** `umap_visualization.py`  
**Output:** `embedding_analysis/all_embeddings_combined_umap.png`

---

## ğŸ“ Folder Structure

vlm-stroop-conflict/
â”‚
â”œâ”€â”€ stroop_images/
â”‚ â”œâ”€â”€ red/
â”‚ â”‚ â”œâ”€â”€ red_as_blue.png
â”‚ â”‚ â”œâ”€â”€ red_as_red.png
â”‚ â”‚ â””â”€â”€ ...
â”‚ â””â”€â”€ blue/
â”‚ â”œâ”€â”€ blue_as_red.png
â”‚ â””â”€â”€ ...
â”‚
â”œâ”€â”€ results/
â”‚ â”œâ”€â”€ clip_embedding_results.csv
â”‚ â””â”€â”€ clip_stroop_umap.png
â”‚
â”œâ”€â”€ embedding_analysis/
â”‚ â””â”€â”€ all_embeddings_combined_umap.png
â”‚
â”œâ”€â”€ clip_stroop_embedding_analysis.py
â”œâ”€â”€ umap_visualization.py
â””â”€â”€ README.md


---

## ğŸ“Š Quantitative Results (`clip_embedding_results.csv`)

Example summary:

| Condition   | Word Bias (%) | Ink Bias (%) | Other (%) |
|--------------|----------------|---------------|------------|
| Congruent    | 100.0 | 0.0 | 0.0 |
| Incongruent  | 100.0 | 0.0 | 0.0 |
| **Overall**  | **100.0** | **0.0** | **0.0** |

**Interpretation:**
> CLIP embeddings are *always* closer to the written word than to the ink color â€” even in incongruent cases.  
> This indicates a strong **word-dominance bias**: CLIP â€œreadsâ€ instead of â€œseesâ€.

---

## ğŸ§  Geometric Results (`all_embeddings_combined_umap.png`)

![UMAP Projection](all_embeddings_combined_umap.png)

Each line connects an image embedding (â—) to its text embedding (â– ) of the same written word.  
Despite conflicting ink colors, image embeddings cluster tightly near their **word** clusters (e.g., â€œREDâ€ â†’ red cluster), not by ink color.

**Observation:**
- Word clusters are compact and separable in the embedding space.  
- Ink information contributes weakly â€” embeddings rarely align with the ink color.

---

## ğŸ§® Interpretation

| Aspect | Observation | Implication |
|--------|--------------|-------------|
| **Behavioral Bias** | 100% alignment to written word | CLIP encodes the *text meaning* seen in the image |
| **Latent Geometry** | Image embeddings cluster with word embeddings | Word concepts are geometrically dominant |
| **Color Representations** | Weak, diffuse | Ink color signals are not clearly separable |

Together, these findings reveal that CLIPâ€™s multimodal space prioritizes **reading** (semantic decoding of text in the image) over **seeing** (color-based perception).

---

## âš™ï¸ How to Reproduce

### 1. Install Dependencies
```bash
pip install torch torchvision torchaudio
pip install openai-clip
pip install umap-learn
pip install matplotlib scikit-learn pandas pillow tqdm

2. Run Analyses
python clip_stroop_embedding_analysis.py
python umap_visualization.py
