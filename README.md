#  VLMRepo â€” Stroop-Style Multimodal Conflict in Visionâ€“Language Models

### Repository Overview

This repository reproduces the experiments and analyses presented in:

> **"What is the Color of RED? Visionâ€“Language Models Prefer to Read Rather Than See"**  
> _Under review at ICLR 2026_  
> **NurbÃ¼ke Teker**, Technical University of Munich (M.Sc. Informatics Thesis, 2025)

The project investigates how **Visionâ€“Language Models (VLMs)** resolve conflicting cues between written words and their ink colors â€” adapting the **classical Stroop paradigm** from psychology.  
Our findings reveal a striking **textual dominance**: when the word and its ink color conflict, VLMs â€œreadâ€ the image rather than â€œseeâ€ it.

---

##  Research Goals

- Adapt the **Stroop test** to multimodal models (CLIP, SigLIP-2, LLaVA, BLIP-2, Kosmos-2, Qwen-VL, etc.)
- Quantify **word vs. color bias** behaviorally and representationally
- Analyze **embedding-space geometry** via RDMs and UMAP
- Apply **subpopulation-based latent interventions** (word vs. color chunks)
- Demonstrate how **representation saliency â‡’ steerability**

---

##  Repository Structure

```bash
VLMRepo/
â”‚
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ stroop_images/           # 100 balanced Stroop stimuli (10 colors Ã— 10 words)
â”‚
â”œâ”€â”€ behavioural_analysis/
â”‚   â”œâ”€â”€ clip_stroop_test.py      # CLIP & SigLIP-2 behavioural evaluation
â”‚   â”œâ”€â”€ generative_vlm_eval.py   # LLaVA, BLIP-2, Kosmos-2, etc.
â”‚   â””â”€â”€ results/                 # Aggregated CSVs and plots
â”‚
â”œâ”€â”€ embedding_analysis/
â”‚   â”œâ”€â”€ rdm_analysis.py          # Representational Dissimilarity Matrices
â”‚   â”œâ”€â”€ umap_projection.py       # Low-dimensional visualization
â”‚   â””â”€â”€ plots/                   # Saved RDM & UMAP figures
â”‚
â”œâ”€â”€ interventions/
â”‚   â”œâ”€â”€ utils_intervention.py    # Model loading & embedding utilities
â”‚   â”œâ”€â”€ llava_color_intervention.py  # Latent steering experiments
â”‚   â””â”€â”€ results/                 # Intervention success heatmaps
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ vlm_stroop_summary.ipynb # End-to-end demonstration notebook
â”‚
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ fig1_stroop_scheme.png
â”‚   â”œâ”€â”€ fig2_behavioral_results.png
â”‚   â”œâ”€â”€ fig4_rdm_umap.png
â”‚   â”œâ”€â”€ fig5_intervention_pipeline.png
â”‚   â””â”€â”€ fig6_intervention_heatmaps.png
â”‚
â””â”€â”€ README.md                    # (this file)

```
## ğŸ“Š Methodology

### 1ï¸âƒ£ Stroop Dataset

100 synthetic images combining 10 color words and 10 ink colors:  
`red, blue, green, yellow, orange, purple, pink, brown, gray, black`.  
Each image is either **congruent** (word = ink) or **incongruent** (word â‰  ink).

Each file follows the format:

<WORD>_<INK>.png


e.g., BLUE_RED.png


<p align="center">
  <img src="figures/fig1_stroop_scheme.png" width="600">
</p>

---

### 2ï¸âƒ£ Behavioural Analysis

#### Contrastive Models (CLIP, SigLIP-2)

We compute the cosine similarity between image and text embeddings:

$$
s(I, T) = \frac{f(I) \cdot g(T)}{\|f(I)\| \|g(T)\|}
$$

| Model    | Word = Ink (%) | Word Acc. (%) | Ink Acc. (%) |
|:----------|:---------------:|:---------------:|:---------------:|
| **CLIP**     | 100.0 | 97.8 | 20.0 |
| **SigLIP-2** | 100.0 | 100.0 | 5.6 |

<p align="center">
  <img src="figures/fig2_behavioral_results.png" width="600">
</p>

#### Generative Models (LLaVA, BLIP-2, Kosmos-2, GIT, Qwen-VL)

When asked *â€œWhat color is the word in this image?â€*,  
models overwhelmingly return the **word** instead of the ink color.

**LLaVA** shows partial correction (â‰ˆ 54 % Ink Match),  
while **Kosmos-2** and **BLIP-2** are strongly word-biased.

---

### 3ï¸âƒ£ Representation Analysis

We compute **Representational Dissimilarity Matrices (RDMs)**  
to isolate modality-specific contributions:

$$
\Delta_{\text{Word}} = \text{RDM(Word + Ink)} - \text{RDM(Ink-only)}
$$

$$
\Delta_{\text{Ink}} = \text{RDM(Word + Ink)} - \text{RDM(Word-only)}
$$

Adding the **word** reshapes the embedding space more sharply than adding the **color**.

<p align="center">
  <img src="figures/fig4_rdm_umap.png" width="600">
</p>

---

### 4ï¸âƒ£ Latent Interventions

Subpopulation-based steering modifies embeddings as:

$$
E' = E - \mu_{\text{src}} + \mu_{\text{tgt}}
$$

| Intervention Type | Success (%) | Mean Î” (sim shift) |
|:------------------|:------------:|:------------------:|
| **Ink Color** | 36.7 | âˆ’0.0017 |
| **Word** | 100.0 | +0.0934 |
| **Combined** | 100.0 | +0.1172 |

<p align="center">
  <img src="figures/fig6_intervention_heatmaps.png" width="600">
</p>

---

##  Key Findings

| Aspect | Observation |
|:-------|:-------------|
| **Behavioral** | VLMs â€œreadâ€ rather than â€œseeâ€ under conflict. |
| **Representation** | Word cues form stronger, separable clusters. |
| **Intervention** | Word chunks steer embeddings reliably; color chunks fail. |
| **Generalization** | Text bias persists across fonts, contrasts, and pseudowords. |

---

##  Usage

```bash
git clone https://github.com/<yourname>/VLMRepo.git
cd VLMRepo
pip install -r requirements.txt

```

### Run the main evaluations:

```bash
python behavioural_analysis/clip_stroop_test.py
python interventions/llava_color_intervention.py

```



## Citation

@article{teker2026vlmstrop,
  title   = {What is the Color of RED? Vision--Language Models Prefer to Read Rather Than See},
  author  = {NurbÃ¼ke Teker},
  journal = {Under review at ICLR},
  year    = {2026}
}


## Acknowledgments

Developed as part of the M.Sc. Informatics Thesis
at Technical University of Munich (TUM) under the supervision of Dr. Shuchen Wu.
Based on open-source implementations of CLIP, SigLIP-2, LLaVA, and BLIP-2.






