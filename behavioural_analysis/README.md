# ğŸ§  CLIP Stroop-Style Behavioral Analysis

This directory contains behavioral evaluation scripts for reproducing **Stroop-style multimodal conflict experiments** on **CLIP**.  
The goal is to analyze whether CLIP prioritizes **textual meaning** or **visual color** when presented with conflicting cues.

---

## ğŸ“ Dataset Overview

We use a **synthetic Stroop dataset** of **100 images**, systematically pairing 10 color words  
(`red, blue, green, yellow, orange, purple, brown, pink, gray, black`)  
with 10 corresponding ink colors.

Each image follows the filename pattern:

```text
<word>/<word>as<color>.png
```

### Example Structure


```text
stroop_images/
â”‚
â”œâ”€â”€ red/
â”‚   â”œâ”€â”€ red_as_red.png
â”‚   â”œâ”€â”€ red_as_blue.png
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ blue/
    â”œâ”€â”€ blue_as_blue.png
    â”œâ”€â”€ blue_as_red.png
    â””â”€â”€ ...

```


| Image File | Word | Ink Color | Congruency |
|-------------|-------|-----------|-------------|
| `red/red_as_red.png` | RED | red | âœ… Congruent |
| `blue/blue_as_green.png` | BLUE | green | âŒ Incongruent |

```text
Each color folder contains 10 PNG images, one for every possible color pairing (10Ã—10 = 100 total images).
```


## ğŸ§© Experimental Goal

```text
The Stroop paradigm tests **modality dominance** in Vision-Language Models â€” whether the model â€œreadsâ€ or â€œseesâ€ more strongly.
```

| Prompt Type | Example Prompt | Focus |
|--------------|----------------|--------|
| **Word-Oriented** | â€œThe text says BLUE.â€ | Semantic meaning (word identity) |
| **Ink-Oriented** | â€œThe text is written in blue color.â€ | Visual appearance (font color) |

```text
By comparing model accuracy under **congruent** (word = color)  
and **incongruent** (word â‰  color) stimuli,  
we reveal whether CLIP favors textual or visual information when these conflict.
```


## âš™ï¸ Running the Analysis

Make sure you have the environment set up (Python â‰¥ 3.10, CUDA optional):

```bash
conda activate vlm-stroop-conflict
pip install torch torchvision transformers pillow pandas tqdm scikit-learn
```


Then run any of the scripts below:

1ï¸âƒ£ Word-Oriented Evaluation
python behavioural_analysis/clip_word_oriented_analysis.py


â†’ Uses prompts like â€œThe text says BLUE.â€

2ï¸âƒ£ Ink-Oriented Evaluation
python behavioural_analysis/clip_ink_oriented_analysis.py


â†’ Uses prompts like â€œThe text is written in blue color.â€

3ï¸âƒ£ Mixed (Prompt-Similarity) Evaluation
python behavioural_analysis/clip_behavioural_analysis.py


â†’ Compares both word- and color-oriented prompt embeddings on all 100 stimuli.

All results are automatically saved as:

behavioural_analysis/results/clip_stroop_results.csv



ğŸ“Š Example Results

Typical results (reproduced using openai/clip-vit-base-patch32):


| Condition       | Prompt Type | Accuracy |
| --------------- | ----------- | -------- |
| **Congruent**   | Word        | 1.000    |
| **Congruent**   | Ink         | 1.000    |
| **Incongruent** | Word        | 1.000    |
| **Incongruent** | Ink         | 0.089    |





ğŸ§  Interpretation

âœ… Congruent cases: CLIP correctly recognizes both word and ink color.

âŒ Incongruent cases: CLIP overwhelmingly follows the word rather than the color.

This reproduces the Stroop-style pattern reported in the thesis:

CLIP â€œreadsâ€ instead of â€œseesâ€ â€” it prioritizes textual semantics even when instructed to focus on color.

ğŸ–¼ï¸ Example Visualization
Image	Color-Prompt	Text-Prompt	CLIP Prediction

	â€œThe text is written in red color.â€ âœ…	â€œThe text says RED.â€ âœ…	Both correct

	â€œThe text is written in green color.â€ âŒ	â€œThe text says BLUE.â€ âœ…	Word-dominant
ğŸ“‚ Output Files
File	Description
clip_word_oriented_analysis.py	Evaluates text-oriented prompts (â€œThe text says Xâ€)
clip_ink_oriented_analysis.py	Evaluates ink-oriented prompts (â€œThe text is written in X colorâ€)
clip_behavioural_analysis.py	Mixed evaluation comparing both modalities
results/clip_stroop_results.csv	Output CSV containing per-image predictions & accuracy
ğŸ§¾ Citation

If you use this dataset or evaluation framework, please cite:

Teker, N. (2025). When VLMs Read Instead of See: Text Dominance in Multimodal Conflict.
Technical University of Munich, Department of Informatics.

ğŸ” Key Takeaway

Across all tests, CLIP consistently shows a strong text dominance bias:

When text and color conflict, it almost always â€œreadsâ€ the word.

Even with explicit color prompts, visual color recognition remains minimal (~9%).

This behavioral signature aligns perfectly with Stroop-style interference effects in cognitive psychology â€” revealing that CLIP, like humans, struggles to suppress its dominant modality (language) when faced with conflicting multimodal cues.










